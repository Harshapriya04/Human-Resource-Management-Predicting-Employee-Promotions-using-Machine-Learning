# -*- coding: utf-8 -*-
"""promotion prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1HB6nWzpbI4YCKV9aTVTIh80Ivh-Kwk
"""
# Importing the libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import cross_val_score
import joblib
#Loading the data set
df=pd.read_csv('emp_promotion.csv')
print('Shape of train data {}'.format(df.shape))
df.head(3)

#Univariate Analysis
plt.figure(figsize=(10,4))
plt.subplot(121)
sns.countplot(x='is_promoted', data=df)
plt.subplot(122)
df['is_promoted'].value_counts().plot(kind='pie', autopct = '%.2f%%', shadow=True)
plt.show()

plt.figure(figsize=(16,10))

plt.subplot(231)
plt.axis('off')
plt.title('KPIs met >80%')
df['KPIs_met >80%'].value_counts().plot(kind='pie', shadow=True, autopct='%.2f%%')

plt.subplot(232)
plt.axis('off')
plt.title('awards won?')
df['awards_won?'].value_counts().plot(kind='pie', shadow=True, autopct='%.2f%%')

plt.subplot(233)
plt.axis('off')
plt.title('previous_year_rating')
df['previous_year_rating'].value_counts().plot(kind='pie', shadow=True, autopct='%.2f%%')

plt.show()



#Length of services column has outliers
plt.figure(figsize=(14,6))
plt.subplot(121)
sns.boxplot(df['length_of_service'], color='g')
plt.subplot(122)
sns.boxplot(df['avg_training_score'], color='g')
plt.show()

#Multivariate Analysis
"""From the below bar plot, we came to know that employee with training score > 95 & previous year rating > 3 got promoted."""

plt.figure(figsize=(20,6))
sns.barplot(x='avg_training_score', y='previous_year_rating', hue='is_promoted', data=df)
plt.title('Promotion Status vs. Training Score and Previous Year Rating')
plt.show()

#Descriptive Analysis

df.describe(include='all')

#Data Preprocessing
"""To predict the promotion, employee id is not required and even sex feature is also not important. For promotion, region and recruitment channel is not important. So, removing employee id, sex, recruitment_channel and region"""

df=df.drop(['employee_id', 'gender', 'region', 'recruitment_channel'], axis = 1)

#Checking for null values
df.isnull().sum()

#Replacing nan with mode
print(df['education'].value_counts())
df['education'] = df['education'].fillna (df['education'].mode()[0])

#Replacing nan with mode
print(df['previous_year_rating'].value_counts())
df['previous_year_rating'] = df['previous_year_rating'].fillna (df['previous_year_rating'].mode()[0])

#Finding the employee who got promoted even in poor performance. It affect model performance.
negative=df[(df['KPIs_met >80%']==0) & (df['awards_won?']==0) & (df['previous_year_rating']==1.0) & (df['is_promoted']==1) & (df['avg_training_score'] <60)]
negative

#Removing negative data
df.drop(index=[31860,51374], inplace=True)

# Handling outliers
"""Outliers are present in length_of_service column"""

q1= np.quantile (df['length_of_service'], 0.25)
q3= np.quantile (df['length_of_service'], 0.75)
IQR = q3-q1
upperBound = (1.5*IQR)+q3
lowerBound = (1.5*IQR)-q1
print('q1:',q1)
print('q3:',q3)
print('IQR:', IQR)
print('Upper Bound:', upperBound)
print('Lower Bound:', lowerBound)
print('Skewed data:',len(df[df['length_of_service']>upperBound]))

""" Here outliers can't be removrd. employee with higher length of services has higher promotion percentage. So, capping is done on this feature."""
pd.crosstab([df['length_of_service']>upperBound], df['is_promoted'])

#capping
df['length_of_service']=[upperBound if x>upperBound else x for x in df['length_of_service']]

# Feature mapping is done on education column
df['education']=df['education'].replace(("Below Secondary", "Bachelor's", "Master's & above"), (1,2,3)).infer_objects(copy=False)


#Label encoding is done on department column
lb = LabelEncoder()
df['department']=lb.fit_transform(df['department'])

#Splitting data and resampling it
x = df.drop('is_promoted', axis=1)
y = df['is_promoted']
print(x.shape)
print(y.shape)


sm =SMOTE()
x_resample, y_resample = sm.fit_resample(x,y)

x_train, x_test, y_train, y_test = train_test_split(x_resample,y_resample, test_size=0.3, random_state=45)
print('Shape of x_train {}'.format(x_train.shape))
print('Shape of y_train {}'.format(y_train.shape))
print('Shape of x_test {}'.format(x_test.shape))
print('Shape of y test {}'.format(y_test.shape))

#Decisiontree Classifier
def decisionTree(x_train, x_test, y_train, y_test):
  dt = DecisionTreeClassifier()
  dt.fit(x_train,y_train)
  yPred = dt.predict(x_test)
  print('***Decision TreeClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test,yPred))
  print('Classification report')
  print(classification_report(y_test,yPred))

#Randomforest Classifier
def randomForest(x_train, x_test, y_train, y_test):
  rf = RandomForestClassifier()
  rf.fit(x_train,y_train)
  yPred = rf.predict(x_test)
  print('***Random ForestClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test,yPred))
  print('Classification report')
  print(classification_report(y_test,yPred))

#KNN Classifier
def knnClassifier(x_train, x_test, y_train, y_test):
  knn = KNeighborsClassifier()
  knn.fit(x_train, y_train)
  yPred = knn.predict(x_test)
  print('***KNeighborsClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test, yPred))
  print('Classification report')
  print(classification_report(y_test, yPred))

#Xgboost Classifier
def xgboost(x_train, x_test, y_train, y_test):
  xg = GradientBoostingClassifier()
  xg.fit(x_train,y_train)
  yPred = xg.predict(x_test)
  print('***GradientBoostingClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test,yPred))
  print('Classification report')
  print(classification_report(y_test,yPred))

#comparingthemodel
def compareModel (x_train, x_test, y_train, y_test):
  decisionTree(x_train, x_test, y_train, y_test)
  print('-'*100)
  randomForest(x_train, x_test, y_train, y_test)
  print('-'*100)
  knnClassifier(x_train, x_test, y_train, y_test)
  print('-'*100)
  xgboost(x_train, x_test, y_train, y_test)

#modelreport
compareModel(x_train, x_test, y_train, y_test)

# Random forest model is selected
rf = RandomForestClassifier()
rf.fit(x_train,y_train)
yPred = rf.predict(x_test)
#cross validation
cv = cross_val_score(rf,x_resample,y_resample, cv = 5 )
np.mean(cv)

#downloadingmodel
filename = 'model.joblib'
joblib.dump(rf, filename,compress=3)
print(f"✅ Model successfully saved to {filename}")

# ✅ Test loading to verify
loaded_model = joblib.load(filename)
print("✅ Model successfully loaded and ready for use!")